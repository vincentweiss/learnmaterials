#数学之美

@(NLP)[Mathematics]

#### **`1. 统计语言模型`**

如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道 S 在文本中出现的可能性，也就是数学上所说的 S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是 P(S)可展开为：

`P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1)`

其中 P (w1) 表示第一个词 w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词 wn，它的出现概率取决于它前面所有词。
从计算上来看，各种可能性太多，无法实现。因此我们 假定任意一个词 i wi  的出现概率只同它前面的词 `wi-1`  有关( ( 即**`马尔可夫假设`**），于是问题就变得很简单了。现在，S 出现的概率
就变为：
`P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)…`
(当然，也可以假设一个词又前面 N-1 个词决定，模型稍微复杂些。）

接下来的问题就是如何估计` P (wi|wi-1)`。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词`（wi-1,wi)` 在统计的文本中出现了多少次，以及 `wi-1` 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,`P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)`。

#### **`2. 中文分词`**

例如把句子 `“中国航天官员应邀到美国与太空总署官员开会。”`
分成一串词：
`中国 / 航天 / 官员 / 应邀 / 到 / 美国 / 与 / 太空 / 总署 / 官员 / 开会。`
最容易想到的，也是最简单的分词办法就是`查字典`。这种方法最早是由北京航天航空大学的梁南元教授提出的。

分成数量最少的词串:二义性 ：对短语 “发展中国家” 正确的分割是“发展-中-国家”，而从左向右查 字典的办法会将它分割成“发展-中国-家”

最长匹配：比如“上海大学城书店”的正确分词应该是 “上海-大学城-书店，” 而不是 “上海大学-城-书店”。

我们假定一个句子 S 可以有几种分词方法，为了简单起见我们假定有以下三种：
`A1, A2, A3, ..., Ak,
B1, B2, B3, ..., Bm
C1, C2, C3, ..., Cn`
其中，A1, A2, B1, B2, C1, C2 等等都是汉语的词。那么 最好的一种分词方法应该保证
分完词后这个句子出现的概率最大。也就是说如果 `A1,A2,..., Ak` 是最好的分法，那么 （P 表
示概率）：
`P (A1, A2, A3, ..., Ak） 〉 P (B1, B2, B3, ..., Bm), 并且
P (A1, A2, A3, ..., Ak） 〉 P(C1, C2, C3, ..., Cn)`

因此，只要我们利用上回提到的统计语言模型计算出每种分词后句子出现的概率，并找出其中概率最大的，我们就能够找到最好的分词方法。

如果我们`穷举所有可能的分词方法`并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，我们可以把它看成是一个`动态规划（Dynamic Programming)` 的问题，并利用 `“维特比”（Viterbi） 算法`快速地找到最佳分词。

需要指出的是，语言学家对词语的定义不完全相同。比如说 “北京大学”，有人认为是一个词，而有人认为该分成两个词。一个折中的解决办法是在分词的同时，找到复合词的嵌套结构。在上面的例子中，如果一句话包含“北京大学”四个字，那么先把它当成一个四字词，然后再进一步找出细分词 “北京” 和 “大学”。

一般来讲， 根据不同应用，汉语分词的 颗粒度 大小应该不同 。比如，`在机器翻译中，颗粒度应该大一些`，“北京大学”就不能被分成两个词。`而在语音识别中，“北京大学”一般是被分成两个词`。

#### **`3. 隐含马尔可夫模型（Hidden Markov Model）`**

一个典型的通信系统：
![Alt text](img/1460713372331.png)
其中 s1，s2，s3...表示信息源发出的信号。o1, o2, o3 ... 是接受器接收到的信号。通信中的解码就是根据接收到的信号 o1, o2, o3 ...还原出发送的信号 s1，s2，s3...。

我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在`已知 o1,o2,o3,...`的情况下，求使得条件概率
`P (s1,s2,s3,...|o1,o2,o3....)` 达到`最大值的那个句子 s1,s2,s3,...`

当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成: 

`P(o1,o2,o3,...|s1,s2,s3....) * P(s1,s2,s3,...)`

其中
`P(o1,o2,o3,...|s1,s2,s3....)` 表示某句话 s1,s2,s3...被读成 o1,o2,o3,...的可能性, 而`P(s1,s2,s3,...)` 表示字串 s1,s2,s3,...本身能够`成为一个合乎情理的句子的可能性`，所以这个公式的意义是用发送信号为 s1,s2,s3...这个数列的可能性乘以 s1,s2,s3...本身可以一个句子的可能性，得出概率。

我们在这里做**`两个假设`**：

第一，`s1,s2,s3,... 是一个马尔可夫链`，也就是说，`si 只由 si-1 决定` 

第二， 第 i 时刻的接收信号 oi 只由发送信号 si 决定（又称为`独立输出假设`, 即
`P(o1,o2,o3,...|s1,s2,s3....) = P(o1|s1) * P(o2|s2)*P(o3|s3)...`

可以很容易利用算法 `Viterbi` 找出上面式子的最大值，进而找出要识别的句子
s1,s2,s3,...。

满足上述两个假设的模型就叫`隐含马尔可夫模型`。我们之所以用`“隐含”`这个词，`是因为状态 s1,s2,s3,...是无法直接观测到的`。

隐含马尔可夫模型的应用远不只在语音识别中。在上面的公式中，如果我们把 s1,s2,s3,...当成中文，把 o1,o2,o3,...当成对应的英文，那么我们就能利用这个模型解决机器翻译问题； 如果我们把 o1,o2,o3,...当成扫描文字得到的图像特征，就能利用这个模型解决印刷体和手写体的识别。

`P (o1,o2,o3,...|s1,s2,s3....)` 根据应用的不同而又不同的名称，在语音识别中它被称为`“声学模型” (Acoustic Model)`， 在机器翻译中是`“翻译模型” (Translation Model) `而在拼写校正中是`“纠错模型” (Correction Model)`。 而` P (s1,s2,s3,...) `就是我们在系列一中提到的语言模型。


#### **`4. 怎样度量信息`**

**“信息熵”**: 信息量的度量就等于不确定性的多少

用 `“比特”（bit）`这个概念来度量信息量。 一个比特是一位二进制数，计算机中的一个字节是八个比特。在上面的例子中，这条消息的信息量是五比特。
（如果有朝一日有六十四个队进入决赛阶段的比赛，那么“谁世界杯冠军”的信息量就是六
比特，因为我们要多猜一次。）

信息量的比特数和所有可能情况的`对数函数 log` 有关。 (`log32=5`, `log64=6`。）

象巴西、德国、意大利这样的球队得冠军的可能性比日本、美国、韩国等队大的多。因此，我们第一次猜测时不需要把 32 个球队等分成两个组，而可以把少数几个最可能的球队分成一组，把其它队分成另一组。然后我们猜冠军球队是否在那几只热门队中。我们重复这样的过程，根据夺冠概率对剩下的候选球队分组，直到找到冠军队。这样，我们也许三次或四次就猜出结果。
因此，当每个球队夺冠的可能性（概率）不等时，“谁世界杯冠军”的信息量的信息量比五
比特少。香农指出，它的`准确信息量应该是
= -（p1*log p1 + p2 * log p2 + ．．． ＋p32 *log p32)`，

其中，`p1，p2 ， ．．．，p32 分别是这 32 个球队夺冠的概率`。香农把它称为`“信息熵”`
(Entropy)，一般用`符号 H `表示，单位是比特。 32 个球队夺冠概率相同时，对应的信息熵等于五比特。

![Alt text](./1460884151848.png)

`变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大`。

`一本五十万字的中文书平均有多少信息量`。我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字。但汉字的使用是不平衡的。
实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而
只考虑每个汉字的独立的概率，那么，`每个汉字的信息熵大约也只有 8-9 个比特`。`如果我
们再考虑上下文相关性，每个汉字的信息熵只有 5 比特左右`。所以，一本五十万字的中文书，信息量大约是 250 万比特。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。`这两个数量的差距，在信息论中称作**“冗余度”（redundancy)**。` 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。

#### **`5. 简单之美：布尔代数和搜索引擎的索引`**

`世界上不可能有比二进制更简单的计数方法了，也不可能有比布尔运算更简单的运算了`

布尔代数简单得不能再简单了。运算的元素只有两个 `1 （TRUE， 真)` 和` 0
（FALSE，假)`。基本的运算只有`“与”（AND)`、`“或” (OR)` 和`“非”（NOT)` 三种（后
来发现，这三种运算都可以转换成“与”“非” ＡＮＤ－ＮＯＴ一种运算）。全部运算只
用下列几张真值表就能完全地描述清楚。

```
AND | 1 0
-----------------------
1 | 1 0
0 | 0 0
```

```
OR | 1 0
-----------------------
1 | 1 1
0 | 1 0
```

```
NOT |
--------------
1 | 0
0 | 1
```

现在我们看看文献检索和布尔运算的关系。对于一个用户输入的关键词，搜索引擎要判断每
篇文献`是否含有这个关键词`，如果一篇文献含有它，我们相应地给这篇文献一个逻辑值 --
真（TRUE,或 1），否则，给一个逻辑值 -- 假（FALSE, 或 0）。比如我们要找有“关原子
能应用”的文献，但并不想知道如何造原子弹。我们可以这样写一个查询语句“原子能 AND
应用 AND (NOT 原子弹)”，表示符合要求的文献必须同时满足三个条件：
- 包含原子能
- 包含应用
- 不包含原子弹

最简单`索引的结构是`用`一个很长的 二进制数表示一个关键字是否出现在每篇文献中。有多少篇文献，就有多少位数，每一位对应一篇文献`，`1  代表相应的文献有这个关键字，0  代表没有。比如关键字“原子能”对应的二进制数是 0100100001100001...，表示第二、第五、第九、第十、第十六篇文献包含着个关键字`。注意，这个二进制数非常之长。同样，我们假定“应用”对应的二进制数是 0010100110000001...。那么要找到同时包含“原子能”和“应用”的文献时，只要`将这两个二进制数进行布尔运算 AND`。根据上面的真值表，我们知道运算结果是 0000100000000001...。表示第五篇，第十六篇文献满足要求。

#### **`6. 图论和网络爬虫(Web Crawlers)`**

`离散数学`是当代数学的一个重要分支，也是计算机科学的数学基础。它包括数理逻辑、集
合论、图论和近世代数四个分支。数理逻辑基于布尔运算

`如何自动下载互联网所有的网页呢，它要用到图论中的遍历（Traverse) 算法`

图论的起源可追溯到大数学家欧拉（Leonhard Euler）。1736 年欧拉来到德国的哥尼斯堡（Konigsberg，大哲学家康德的故乡，现在是俄罗斯的加里宁格勒），发现当地市民们有一项消遣活动，就是试图将下图中的每座桥恰好走过一遍并回到原出发点，从来没有人成功过。欧拉证明了这件事是不可能的，并写了一篇论文，一般认为这是图论的开始。

![Alt text](./1460973575704.png)

图论中所讨论的的图由一些节点和连接这些节点的弧组成。如果我们把中国的城市当成节点，连接城市的国道当成弧，那么全国的公路干线网就是图论中所说的图。

以中国公路网为例，我们从北京出发，看一看北京和哪些城市直接相连，比如说和天津、济南、石家庄、南京、沈阳、大同直接相连。我们可以依次访问这些城市，然后我们看看都有哪些城市和这些已经访问过的城市相连，比如说北戴河、秦皇岛与天津相连，青岛、烟台和济南相连，太原、郑州和石家庄相连等等，我们再一次访问北戴河这些城市，直到中国所有的城市都访问过一遍为止。这种图的遍历算法称为`“广度优先算法”（BFS)`，因为它先要`尽可能广地访问每个节点所直接连接的其他节点`。

另外还有一种策略是从北京出发，随便找到下一个要访问的城市，比如是济南，然后从济南出发到下一个城市，比如说南京，再访问从南京出发的城市，一直走到头。`然后再往回找，看看中间是否有尚未访问的城市`。这种方法叫`“深度优先算法”（DFS)`，因为它是一条路走到黑。

现在我们看看图论的遍历算法和搜索引擎的关系。`互联网其实就是一张大图，我们可以把每一个网页当作一个节点，把那些超链接（Hyperlinks)当作连接网页的弧`。

有了超链接，我们可以从任何一个网页出发，用图的遍历算法，自动地访问到每一个网页并把它们存起来。完成这个功能的程序叫做`网络爬虫`，或者在一些文献中称为`"机器人" （Robot)`

在网络爬虫中，我们使用一个称为`“哈希表”(Hash Table)`的列表而不是一个记事本纪录网页是否下载过的信息。

#### **`7. 信息论在信息处理中的应用`**

语言模型是为了用上下文预测当前的文字，`模型越好，预测得越准，那么当前文字的不确定性就越小`。

信息熵正是对不确定性的衡量，因此信息熵可以直接用于衡量统计语言模型的好坏。 `贾里尼克从信息熵出发，定义了一个称为语言模型复杂度 (Perplexity) 的概念`，`直接衡量语言模型的好坏`。`一个模型的复杂度越小，模型越好`。李开复博士在介绍他发明的 Sphinx 语音识别系统时谈到，如果不用任何语言模型（即零元语言模型）时，复杂度为 997，也就是说句子中每个位置有 997 个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配不考虑搭配的概率时，复杂度为 60。虽然它比不用语言模型好很多，但是和考虑了搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有 20。

信息论中仅次于熵的另外两个重要的概念是`“互信息”（Mutual Information) `和`“相对熵”（Kullback-Leibler Divergence)`。

`“互信息”是信息熵的引申概念，它是对两个随机事件相关性的度量`。比如说今天随机事件北京下雨和随机变量空气湿度的相关性就很大，但是和姚明所在的休斯敦火箭队是否能赢公牛队几乎无关。 

`互信息就是用来量化度量这种相关性的。`在自然语言处理中，经常要度量一些语言现象的相关性。比如在机器翻译中，最难的问题是词义的二义性（歧义性）问题。

人们很容易想到要用语法、要分析语句等等。其实，至今为止，`没有一种语法能很好解决这个问题`，`真正实用的方法是使用互信息`。具体的解决办法大致如下：首先从大量文本中找出和总统布什一起出现的互信息最大的一些词，比如总统、美国、国会、华盛顿等等，当然，再用同样的方法找出和灌木丛一起出现的互信息最大的词，比如土壤、植物、野生等等。有了这两组词，在翻译 Bush 时， 看看上下文中哪类相关的词多就可以了。

`“相对熵”`，在有些文献中它被称为成`“交叉熵”`。`相对熵用来衡量两个正函数是否相似，对于两个完全相同的函数，它们的相对熵等于零`。在自然语言处理中可以用相对熵来衡量`两个常用词（在语法上和语义上）是否同义`，或者`两篇文章的内容是否相近`等等。利用相对熵，我们可以到处信息检索中最重要的一个概念： 
**`词频率 - 逆向文档频率（ TF/IDF)`**。我们下回会介绍如何根据相关性对搜索出的网页进行排序，就要用的餐 TF/IDF 的概念。另外，在新闻的分类中也要用到相对熵和 TF/IDF。

#### **`8. 贾里尼克的故事和现代语言处理`**

在贾里尼克以前，科学家们把语音识别问题当作人工智能问题和模式匹配问题。而 贾里尼克把它当成通信问题，并用`两个隐含马尔可夫模型（声学模型和语言模型）`把语音识别概括得清清楚楚。

```
贾格布森的通信模型
1 上下文
2 信息
3 发送着
4 接收者
5 信道
6 编码
```

#### **`9. 如何确定网页和查询的相关性`**

我们知道，短语“原子能的应用”可以分成三个关键词：原子能、的、应用。根据我们的直觉，我们知道，包含这三个词多的网页应该比包含它们少的网页相关。当然，这个办法有一个明显的漏洞，就是长的网页比短的网页占便宜，因为长的网页总的来讲包含的关键词要多些。因此我们需要根据网页的长度，对 `关键词的次数进行归一化`，也就是 `用关键词的次数除以网页的总字数`。我们把这个商称为`“关键词的频率”`，或者`“单文本词汇频率”（ TermFrequency)`

比如，在某个一共有一千词的网页中“原子能”、“的”和“应用”分别出现了 2 次、35 次 和 5 次，那么它们的词频就分别是 0.002、0.035 和 0.005。 我们将`这三个数` 相加，其和 `0.042` 就是相应网页和查询`“原子能的应用”`相关性的一个简单的度量。概括地讲，如果一个查询包含关键词 `w1,w2,...,wN`, 它们在一篇特定网页中的词频分别是:`TF1, TF2, ..., TFN`。 `（TF: term frequency)`。 那么，这个查询和该网页的相关性就是:`TF1 + TF2+ ... + TFN`。

在上面的例子中，词`“的”`站了总词频的 80% 以上，而它对确定网页的主题几乎没有用。我们称这种词叫`“应删除词”（Stopwords)`，也就是说在度量相关性是不应考虑它们的频率。

在汉语中，`“应用”`是个很通用的词，而`“原子能”`是个很`专业`的词，`后者在相关性排名中比前者重要`。 

因此我们需要给汉语中的每一个词给一个`权重`，这个权重的设定必须满足下面两个条件：

1. `一个词预测主题能力越强，权重就越大，反之，权重就越小`。我们在网页中看到“原子能”这个词，或多或少地能了解网页的主题。我们看到“应用”一次，对主题基本上还是一无所知。因此，“原子能“的权重就应该比应用大。

2. `应删除词的权重应该是零。`

我们很容易发现，`如果一个关键词只在很少的网页中出现，我们通过它就容易锁定搜索目标，它的权重也就应该大`。反之如果一个词在大量网页中出现，我们看到它仍然不很清楚要找什么内容，因此它应该小。
概括地讲，假定一个关键词` w `在`  D w `个网页中出现过，那么` D w `越大，`ｗ` 的权重越小，反之亦然。在信息检索中，使用最多的权重是“ `逆文本频率指数`”（` Inverse document frequency`  缩写为 ` IDF` ），它的 公式为  `log( D  / D w )` )其中 `Ｄ` 是`全部网页数`。

利用 IDF，上述 相关性计算个公式就由词频的简单求和变成了加权求和，即 `TF 1 1 *IDF 1 1 + TF 2 2 *IDF 2 2 ＋ ... + TF N N *IDF N N `。

`ＴＦ／ＩＤＦ（ term frequency/inverse document frequency) ` 的概念被公认为信息检索中最重
要的发明。

`IDF`  的概念就是`一个特定条件下、关键词的概率分布的交叉熵（ Kullback- - Leibler Divergence)`（详见上一系列）。这样， 信息检索相关性的度量，又回到了信息论。

#### **`10. 有限状态机和地址识别`**
`地址的识别和分析`是本地搜索必不可少的技术，尽管有许多识别和分析地址的方法，最有效的是`有限状态机`。

一个有限状态机是`一个特殊的有向图`（参见有关图论的系列），它包括一些`状态（节点）`和`连接`这些状态的`有向弧`。

![Alt text](./1460984055182.png)

每一个 有限状态机都有一个启始状态和一个终止状态和若干中间状态。每一条弧上带有从一个状态进入下一个状态的条件。比如，在上图中，当前的状态是“省”，如果遇到一个词组和（区）县名有关，我们就进入状态“区县”；如果遇到的下一个词组和城市有关，那么我们就进入“市”的状态，如此等等。 

`如果一条地址能从状态机的起始状态经过状态机的若干中间状态，走到终止状态，那么这条地址则有效，否则无效。`

使用有限状态机识别地址，关键 `要解决两个问题，即通过一些有效的地址建立状态机，以及给定一个有限状态机后，地址字串的匹配算法`。

上述基于有限状态机的地址识别方法在实用中会有一些问题：当用户输入的地址不太标准或者有错别字时，有限状态机会束手无策，因为它只能进行严格匹配。（其实， `有限状态机在计算机科学中早期的成功应用是在程序语言编译器的设计中`。一个能运行的程序在语法上必须是没有错的，所以不需要模糊匹配。而自然语言则很随意，无法用简单的语法描述。）

为了解决这个问题，我们希望有一个能进行 `模糊匹配`、`并给出一个字串为正确地址的可能性`。为了实现这一目的，科学家们提出了 `基于概率的有限状态机`。`这种基于概率的有限状态机`和`离散的马尔可夫链（详见前面关于马尔可夫模型的系列）`基本上等效。

#### **`12. 余弦定理和新闻的分类`**
和新闻主题有关的那些实
词频率高，TF/IDF 值很大。我们按照这些实词在词汇表的位置对它们的 TF/IDF 值排序。
比如，词汇表有六万四千个词，分别为

```
单词编号 汉字词
------------------
1 阿
2 啊
3 阿斗
4 阿姨
...
789 服装
....
64000 做作
```

```
在一篇新闻中，这 64,000 个词的 TF/IDF 值分别为
单词编号 TF/IDF 值
========  ======
1 0
2 0.0034
3 0
4 0.00052
5 0
...
789 0.034
...
64000 0.075
```
如果单词表中的某个次在新闻中没有出现，对应的值为零，那么这 64,000 个数，`组成一个 64,000 维的向量`。我们就 `用这个向量来代表这篇新闻 ， 并成为新闻的特征向量`。如果两篇新闻的特征向量相近，则对应的新闻内容相似，它们应当归在一类，反之亦然。

`向量实际上是多维空间中有方向的线段`。如果两个向量的方
向一致，即夹角接近零，那么这两个向量就相近。`而要确定两个向量方向是否一致，这就要
用到**[余弦定理](http://baike.baidu.com/link?url=lM2G1WPMHwDQSN08hDELDT0EAgpROPzeSsz3v5I6FB2esgA8uGNuYqMHkWEH6aFgoFJqB1fZLQaNo_FOM__rPK)**计算向量的夹角了。

![Alt text](./1461406962480.png)

如果我们将三角形的两边 b 和 c 看成是两个向量，那么上述公式等价于

![Alt text](./1461407568453.png)

其中分母表示两个向量 b 和 c 的长度，分子表示两个向量的内积。举一个具体的例子，
假如`新闻 X` 和`新闻 Y` 对应向量分别是

`x1,x2,...,x64000 和
y1,y2,...,y64000,`

那么它们夹角的余弦等于，
![Alt text](./1461412193292.png)

当两条新闻向量夹角的余弦等于一时，`这两条新闻完全重复（ 用这个办法可以删除重复
的网页）`；当夹角的余弦接近于一时，两条新闻相似，从而可以归成一类；夹角的余弦越小，
两条新闻越不相关。

![Alt text](./1461418633077.png)


#### **`13. 信息指纹及其应用`**

`任何一段信息文字，都可以对应一个不太长的随机数，作为区别它和其它信息的指纹（Fingerprint)`。只要算法设计的好，任何两段信息的指纹都很难重复，就如同人类的指纹一样。信息指纹在加密、信息压缩和处理中有着广泛的应用。

在图论和网络爬虫一文中提到，为了防止重复下载同一个网页，我们需要在哈希表中纪录已经访问过的网址（URL)。`但是在哈希表中以字符串的形式直接存储网址，既费内存空间，又浪费查找时间。`现在的网址一般都较长，比如，如果在 Google 或者百度在查找数学之美，对应的网址长度在一百个字符以上。下面是百度的链接`http://www.baidu.com/s?ie=gb2312&bs=%CA%FD%D1%A7%D6%AE%C3%C0&sr=&amp;z=&cl=3&f=8&wd=%CE%E2%BE%FC+%CA%FD%D1%A7%D6%AE%C3%C0&ct=0`

假定网址的平均长度为一百个字符，那么存贮 200 亿个网址本身至少需要 2 TB，即两千 GB 的容量，`考虑到哈希表的存储效率一般只有 50%`，实际需要的内存在 4 TB 以上。即使把这些网址放到了计算机的内存中，`由于网址长度不固定，以字符串的形式查找的效率会很低`。因此，我们如果能够找到一个函数，`将这 200 亿个网址随机地映射到 128 二进位即 16 个字节的整数空间`，比如将上面那个很长的字符串对应成一个如下的`随机数`:

`893249432984398432980545454543`

`这样每个网址只需要占用 16 个字节而不是原来的一百个`。这就能把存储网址的内存需求量降低到原来的 1/6。这个 16 个字节的随机数，就称做该`网址`的`信息指纹（Fingerprint)`。

由于指纹是`固定的 128 位整数`，因此查找的计算量比字符串比较小得多。`网络爬虫在下载网页时，它将访问过的网页的网址都变成一个个信息指纹，存到哈希表中`，每当遇到一个新网址时，计算机就计算出它的指纹，然后比较该指纹是否已经在哈希表中，来决定是否下载这个网页。

产生信息指纹的关键算法是`伪随机数产生器算法（prng)`。最早的 prng 算法是由计算机之父冯诺伊曼提出来的。他的办法非常简单，就是将一个数的平方掐头去尾，取中间的几位数。比如一个四位的二进制数 1001 （相当于十进制的 9），其平方为 01010001 (十进制的 81）掐头去尾剩下中间的四位 0100。当然这种方法产生的数字并不很随机，也就是说两个不同信息很有可能有同一指纹。现在常用的` MersenneTwister 算法`要好得多。

信息指纹的一个特征是其不可逆性, 也就是说,无法根据信息指纹推出原有信息，这种性质， 正是网络加密传输所需要的。比如说，`一个网站可以根据用户的 Cookie 识别不同用户，这个 cookie 就是信息指纹`。但是网站无法根据信息指纹了解用户的身份，这样就可以保护用户的隐私。在互联网上，`加密的可靠性，取决于是否很难人为地找到拥有同一指纹的信息`， 比如一个黑客是否能随意产生用户的cookie。

#### **`14. 数学模型的重要性`**

１. 一个正确的数学模型应当在形式上是简单的。（托勒密的模型显然 太复杂。）
２. 一个正确的模型在它开始的时候可能还不如一个精雕细琢过的错误的模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去。（日心说开始并没有地心说准确。）
３. 大量准确的数据对研发很重要。
４. 正确的模型也可能受噪音干扰，而显得不准确；这时我们不应该用一种凑合的修
正方法来弥补它，而是要找到噪音的根源，这也许能通往重大发现。

在网络搜索的研发中，我们在前面提到的`单文本词频/逆文本频率指数（TF/IDF)` 和`网页排名（page rank)`都相当于是网络搜索中的“椭圆模型”，它们都很简单易懂。

#### **`16. 不要把所有的鸡蛋放在一个篮子里 --  谈谈最大熵模型`**

前段时间，Google 中国研究院的刘骏总监谈到在网络搜索排名中，用到的信息有上百种。更普遍地讲，在自然语言处理中，我们常常知道各种各样的但是又不完全确定的信息，我们需要用一个统一的模型将这些信息综合起来。如何综合得好，是一门很大的学问。

让我们看一个拼音转汉字的简单的例子。假如输入的拼音是"wang-xiao-bo"，利用语言模型，根据有限的上下文(比如前两个词)，我们能给出两个最常见的名字“王小波”和“王晓波”。至于要唯一确定是哪个名字就难了，即使利用较长的上下文也做不到。当然，我们知道如果通篇文章是介绍文学的，作家王小波的可能性就较大；而在讨论两岸关系时，台湾学者王晓波的可能性会较大。在上面的例子中，我们只需要综合两类不同的信息，即主题信息和上下文信息。虽然有不少凑合的办法，比如：分成成千上万种的不同的主题单独处理，或者对每种信息的作用加权平均等等，但都不能准确而圆满地解决问题，这样好比以前我们谈到的行星运动模型中的小圆套大圆打补丁的方法。在很多应用中，我们需要综合几十甚至上百种不同的信息，这种小圆套大圆的方法显然行不通。

数学上最漂亮的办法是`最大熵 (maximum entropy) 模型`，它相当于行星运动的椭圆模型。“最大熵”这个名词听起来很深奥，但是它的原理很简单，我们每天都在用。 说白了，`就是要保留全部的不确定性，将风险降 到最小`。让我们来看一个实际例子。


有一次，我去 AT&T 实验室作关于最大熵模型的报告，我带去了一个色子。我问听众“每个面朝上的概率分别是多少”，所有人都说是等概率，即各点的概率均为 1/6。这种猜测当然是对的。我问听众们为什么，得到的回答是一致的： `对这个“一无所知”的色子，假定它。 每一个朝上概率均等是最安全的做法`。（你不应该主观假设它象韦小宝的色子一样灌了铅。）从投资的角度看，就是风险最小的做法。 `从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。`接着，我又告诉听众，我的这个色子被我特殊处理过，已知四点朝上的概率是三分之一，在这种情况下，每个面朝上的概率是多少？这次，大部分人认为除去四点的概率是 1/3，其余的均是 2/15，也就是说已知的条件（四点概率为 1/3）必须满足，而对其余各点的概率因为仍然无从知道，因此只好认为它们均等。注意，在猜测这两种不同情况下的概率分布时，大家都没有添加任何主观的假设，诸如四点的反面一定是三点等等。（事实上，有的色子四点反面不是三点而是一点。） `这种基于直觉的猜测之所以准确，是因为它恰好符合了最大熵原理`。

`最大熵原理指出，当我们需要对一个随机事件的概率分布进 行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设`。（不做主观假设这点很重要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫 `“最大熵模型”`。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。

我们已知两种信息，第一，根据语言模型，wang-xiao-bo 可以被转换成王晓波和王小波；第二，根据主题，王小波是作家，《黄金时代》的作者等等，而王晓波是台湾研究两岸关系的学者。因此，我们就可以建立一个最大熵模型，同时满足这两种信息。现在的问题是，这样一个模型是否存在。匈牙利著名数学家、信息论最高奖香农奖得主 希萨（ Csiszar ）证明， `对任何一组不自相矛盾的信息，这个最大熵模型不仅存在，而且是唯一的`。而且它们都有同一个非常简单的形式 --  `指数函数`。下面公式是根据上下文（前两个词）和主题预测下一个词的最大熵模型，其中 `w3 `是`要预测的词（王晓波或者王小波）``w1 和 w2 是它的前两个字（比如说它们分别是“出版”，和“”）`，也就是其上下文的一个大致估计，`subject `表示`主题`。

![Alt text](./1461505112788.png)

在上面的公式中，有几个参数 lambda 和 Z ，他们需要通过观测数据训练出来。

最原始的最大熵模型的训练方法是一种称为 `通用迭代算法 GIS(generalized iterativescaling) `的`迭代算法`。GIS 的原理并不复杂，大致可以概括为以下几个步骤：
1. 假定第零次迭代的初始模型为等概率的均匀分布。
2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际的，就把相应的模型参数变小；否则，将它们便大。
3. 重复步骤 2 直到收敛。

GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定，即使在 64 位计算机上都会出现溢出。因此，在实际应用中很少有人真正使用 GIS。大家只是通过它来了解最大熵模型的算法。

达拉皮垂(Della Pietra)在 IBM 对 GIS 算法进行了两方面的改进，提出了`改进迭代算法 IIS（improved iterative scaling）`。这使得最大熵模型的训练时间缩短了一到两个数量级。

#### **`17. 谈谈搜索引擎作弊问题`**
抓作弊的方法很像信号处理中的去噪音的办法。消除噪音的流程可以概括如下：

![Alt text](./1461510256266.png)

在图中，原始的信号混入了噪音，在数学上相当于`两个信号做卷积`。`噪音消除的过程是 一个解卷积的过程`。这在信号处理中并不是什么难题。因为第一，汽车发动机的频率是固定 的，第二，这个频率的噪音重复出现，只要采集几秒钟的信号进行处理就能做到。从广义上 讲，`只要噪音不是完全随机的、并且前后有相关性`，就可以检测到并且消除。（事实上，完 全随机不相关的高斯白噪音是很难消除的。） 

#### **`18. 矩阵运算和文本处理中的分类问题`**

在自然语言处理中，最常见的两类的分类问题分别是，将`文本按主题归类`（比如将所有介绍 亚运会的新闻归到体育类）和`将词汇表中的字词按意思归类`（比如将各种体育运动的名称个 归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。

**`分类的关键是计算相关性`**

`当这两个向量夹角为零时，新闻就相关`；`当它 们垂直或者说正交时，新闻则无关`。当然，夹角的余弦等同于向量的内积。从理论上讲，这 种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万 篇以上，`二次回标`有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想 通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对 文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需 要十五年时间。

在文本分类中，另一种办法是`利用矩阵运算中的奇异值分解（Singular Value Decomposition， 简称 SVD)`。

首先，我们可以用一个大矩阵 A来 描述这`一百万篇文章`和`五十万词`的关联性。`这个矩阵中，每一行对应一篇文章`，`每一列对应 一个词`。

![Alt text](./1461576462769.png)

在上面的图中，`M=1,000,000`，`N=500,000`。`第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)`。读者可能已经注意到了，这个矩阵非常大， 有一百万乘以五十万，即五千亿个元素。

`奇异值分解`就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面 的例子中的`矩阵分解成一个一百万乘以一百的矩阵 X，一个一百乘以一百的矩阵 B，和一个 一百乘以五十万的矩阵 Y`。

![Alt text](./1461576821972.png)

三个矩阵有非常清楚的物理含义。第一个矩阵 X 中的`每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关`。最后一 个矩阵 Y 中的每一列表示同一主题一类文章，其中`每个元素表示这类文章中每篇文章的相 关性。中间的矩阵 B 则表示类词和文章之间的相关性`。因此，我们只要对关联矩阵 A进行一 次奇异值分解，我们就可以`同时完成了近义词分类和文章的分类`。（ 同时得到每类文章和每 类词的相关性）。

在很长时间内，奇异 值分解都无法并行处理。（虽然 Google 早就有了 MapReduce 等并行计算的工具，但是由 于奇异值分解`很难拆成不相关子运算`，即使在 Google 内部以前也无法利用并行计算的优势 来分解矩阵。）


#### **`19. 马尔可夫链的 扩展 贝叶斯网络 (Bayesian Networks)`**

马尔可夫链 (Markov Chain)，它描述了一种`状态序列`，`其每个状态值取决于前面有限个状态`。

在现实生活中，很多事物相互的关系并不能用一条 链来串起来。它们之间的关系可能是交叉的、错综复杂的。比如在下图中可以看到，心血管 疾病和它的成因之间的关系是错综复杂的。显然无法用一个链来表示。 

![Alt text](./1461578519903.png)

我们可以把上述的有向图看成一个网络，它就是`贝叶斯网络`。其中每个圆圈表示一个状 态。`状态之间的连线表示它们的因果关系`。比如从心血管疾病出发到吸烟的弧线表示心血管 疾病可能和吸烟有关。这些关系可以有一个量化的可信度 (belief)，用一个概率描述。 我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。`在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名`。由于网络的`每个弧有一个可信度`， 贝叶斯网络也被称作`信念网络 (belief networks)`。

和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝 叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述 事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链 的推广。

使用`贝叶斯网络`必须`知道各个状态之间相关的概率`。`得到这些参数的过程叫做训练`。和 训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据。比如在训练上面的网络，需 要知道一些心血管疾病和吸烟、家族病史等有关的情况。相比马尔可夫链，贝叶斯网络的训 练比较复杂，从理论上讲，它是一个 NP-complete 问题，也就是说，对于现在的计算机是 不可计算的。但是，对于某些应用，这个训练过程可以简化，并在计算上实现。 


#### **`20. 布隆过滤器 （Bloom Filter）`**

一般来讲，计算机中的集合是用哈希表（hash table）来 存储的。它的好处是快速准确，缺点是费存储空间。当集合比较小时，这个问题不显著，但 是当集合巨大时，哈希表存储效率低的问题就显现出来了。比如说，一个象 Yahoo,Hotmail 和  Gmai 那样的公众电子邮件（email）提供商，总是需要过滤来自发送垃圾邮件的人 （spamer）的垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发 送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来 则需要大量的网络服务器。如果用哈希表，每存储一亿个 email 地址， 就需要 1.6GB 的 内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹 googlechinablog.com/2006/08/blog-post.html，然后将这些信息指纹存入哈希表，由于哈希表 的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6GB， 即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB 的内存。 除非是超级计算机，一般服务器是无法存储的。

布隆过滤器的数学工具，它只需要哈希表 1/8 到 1/4 的大小 就能解决同样的问题。

布隆过滤器是由巴顿.布隆于一九七零年提出的。它实际上是一个很长的`二进制向量`和 一系列`随机映射函数`

假定我们存储一亿个电子邮件地址，我们先建立一个十六亿二进制（比特），即两亿字 节的向量，然后将这十六亿个二进制全部设置为零。

对于每一个电子邮件地址 X，我们用 `八个不同`的`随机数产生器（F1,F2, ...,F8）` 产生`八个信息指纹（f1, f2, ..., f8）`。再用一个随 `机数产生器 G` 把这`八个信息指纹映射`到 `1 到十六亿中的八个自然数 g1, g2, ...,g8`。现在我 们把这八个位置的二进制全部设置为一。当我们对这一亿个 email 地址都进行这样的处理 后。一个针对这些 email 地址的布隆过滤器就建成了。（见下图） 

![Alt text](./1461596890968.png)

#### **`22. 谈谈密码学的数学原理`**

公开密钥的原理其实很简单，我们以给上面的单词 Caesar 加解密来说明它的原理。我 们先把它变成一组数，比如它的 Ascii 代码 X=099097101115097114（每三位代表一个字母） 做明码。现在我们来设计一个密码系统，对这个明码加密。   

1，找两个`很大的素数（质数）P 和 Q`，越大越好，比如 100 位长的, 然后计算它们的 乘积 `N=P×Q`，`M=（P-1）×（Q-1）`。

2，找一个和 `M` `互素`的整数 `E`，也就是说 M 和 E 除了 1 以外没有公约数。   

3，找一个整数` D`，使得 `E×D 除以 M 余 1`，即 `E×D mod M = 1`。   

现在，世界上先进的、最常用的密码系统就设计好了，其中 `E 是公钥谁都可以用来加 密`，D 是私钥用于解密，一定要自己保存好。`乘积 N 是公开的，即使敌人知道了也没关系`。    现在，我们用下面的公式对 X 加密，得到密码 Y。

![Alt text](./1461638266021.png)

好了，现在没有密钥 D，神仙也无法从 Y 中恢复 X。如果知道 D，根据费尔马小定 理，则只要按下面的公式就可以轻而易举地从 Y 中得到 X。 

如果知道 D，根据费尔马小定 理，则只要按下面的公式就可以轻而易举地从 Y 中得到 X。

![Alt text](./1461638396648.png)


![Alt text](./1461638440663.png)

#### **`22. 香农第一定律`**

我们假定常用的汉字在二级国标里面，一共有 6700 个作用的汉字。如果不考虑汉字频 率的分布，用键盘上的 26 个字母对汉字编码，两个字母的组合只能对 676 个汉字编码， 对 6700 个汉字编码需要用三个字母的组合，即编码长度为三。当然，聪明的读者马上发现 了我们可以对常见的字用较短的编码对不常见的字用较长的编码，这样平均起来每个汉字的 编码长度可以缩短。

我们假定每一个汉字的频率是 
p1, p2, p3, ..., p6700 
它们编码的长度是 
L1, L2, L3, ..., L6700 
那么，平均编码长度是 
p1×L1 + p2×L2 + ... + p6700×L6700   

`香农第一定理指出：这个编码的长度的最小值是汉字的信息熵，也就是说任何输入方面 不可能突破信息熵给定的极限`。当然，香农第一定理是针对所有编码的，不但是汉字输入编 码的。这里需要指出的是，如果我们将输入法的字库从二级国标扩展到更大的字库 GBK， 由于后面不常见的字频率较短，平均编码长度比针对国标的大不了多少。

信息熵（见 http://www.googlechinablog.com/2006/04/4.html）， 

`H = -p1 * log p1 - ... - p6700 log p6700。`

我们如果对每一个字进行统计，而且不考虑上下文相关性，大致可以估算出它的值在`十 比特`以内，当然这取决于用什么语料库来做估计。如果我们假定输入法只能用 26 个字母输 入，那么每个字母可以代表 `log26= 4.7` 比特的信息，也就是说，输入一个汉字平均需要敲 `10/4.7= 2.1` 次键。 

如果我们把汉字组成词，再以词为单位统计信息熵，那么， 每个汉字的平均信息熵将会减少。这样，平均输入一个字可以少敲零点几次键盘。不考虑词 的上下文相关性，以词为单位统计，汉字的信息熵大约是 8 比特作用，也就是说，以词为单 位输入一个汉字平均只需要敲 8/4.7=1.7 次键。这就是现在所有输入法都是基于词输入的内 在原因。

当然，如果我们再考虑上下文的相关性，对汉语建立一个基于词的统计语言模型（见 http://www.googlechinablog.com/2006/04/blog-post.html），我们可以将每个汉字的信息熵降 到 `6 比特`作用，这时，输入一个汉字只要敲 6/4.7=1.3 次键。如果一种输入方法能做到这 一点，那么汉字的输入已经比英文快的多了。 

汉语全拼的平均长度为 `2.98`，只要基于 拼音的输入法能`利用上下文`彻底解决一音多字的问题，平均每个汉字输入的敲键次数应该在 三次左右，每分钟输入 100 个字完全有可能达到。 

#### **`22. 动态规划`**

今年九月二十三日，Google、 T-Mobile 和  HTC 宣布了第一款基于开源操作系统 Android 的 3G 手机，其中一个重要的功能是利用全球卫星定位系统实现全球导航。这个功 能在其它手机中早已使用，并且早在五六年前就已经有实现这一功能的车载设备出售。其中 的关键技术只有两个：第一是利用卫星定位；第二根据用户输入的起终点，在地图上规划最 短路线或者最快路线。后者的关键算法是计算机科学图论中的`动态规划（Dynamic Programming）的算法`。 

我们想找到从北京到广州的最短行车路线或者最快行车路线。当然，最直接的笨办法是把所 有可能的路线看一遍，然后找到最优的。这种办法只有在节点数是个位数的图中还行得通， 当图的节点数（城市数目）有几十个的时候，计算的复杂度就已经让人甚至计算机难以接受 了，因为所有可能路径的个数随着节点数的增长而成呈指数增长（或者说几何级数），也就 是说每增加一个城市，复杂度要大一倍。

所有的导航系统采用的都是动态规划的办法（Dynamic Programming），这里面的规划 （programming）一词在数学上的含义是“优化”的意思，不是计算机里面编程的意思。它的 原理其实很简单。以上面的问题为例，当我们要找从北京到广州的最短路线时，我们先不妨 倒过来想这个问题：假如我们找到了所要的最短路线（称为路线一），如果它经过郑州，那 么从北京到郑州的这条子路线（比如是北京-> 保定->石家庄->郑州，称为子路线一），必 然也是所有从北京到郑州的路线中最短的。否则的话，我们可以假定还存在从北京到郑州更 短的路线（比如北京->济南->徐州->郑州，称为子路线二），那么只要用这第二条子路线代 替第一条，我们就可以找到一条从北京到广州的全程更短的路线（称为路线二），这就和我 们讲的路线一是北京到广州最短的路线相矛盾。其矛盾的根源在于，我们假设的子路线二或 者不存在，或者比子路线一还来得长。 

在实际实现算法时，我们又正过来解决这个问题，也就是说，要想找到从北京到广州的 最短路线，先要找到从北京到郑州的最短路线。当然，聪明的读者可能已经发现其中的一个 “漏洞”，就是我们在还没有找到全程最短路线前，不能肯定它一定经过郑州。不过没有关系， 只要我们在图上横切一刀，这一刀要保证将任何从北京到广州的路一截二，如下图。

![Alt text](./1461640157303.png)

`那么从广州到北京的最短路径必须经过这一条线上的某个城市（图中蓝色的菱形）`。`我 们可以先找到从北京出发到这条线上所有城市的最短路径，最后得到的全程最短路线一定包 括这些局部最短路线中的一条，这样，我们就可以将一个“寻找全程最短路线”的问题，分解 成一个个小的寻找局部最短路线的问题`。只要我们将这条横切线从北京向广州推移，直到广 州为止，我们的全程最短路线就找到了。这便是动态规划的原理。采用动态规划可以大大降 低最短路径的计算复杂度。在我们上面的例子中，每加入一条横截线，线上平均有十个城市， 从广州到北京最多经过十五个城市，那么采用动态规划的计算量是 10×10×15，而采用穷举 路径的笨办法是 10 的 15 次方，前后差了万亿倍。 

那么动态规划和我们的拼音输入法又有什么关系呢？`其实我们可以将汉语输入看成一 个通信问题，而输入法则是一个将拼音串到汉字串的转换器。每一个拼音可以对应多个汉字， 一个拼音串就可以对应图论中的一张图`，如下：

![Alt text](./1461640469207.png)

其中，Y1,Y2,Y3,……,YN 是使用者输入的拼音串，W11,W12,W13 是第一个音 Y1 的 候选汉字，W21,W22,W23,W24 是对应于 Y2 的候选汉字，以此类推。从第一个字到最后 一个字可以组成很多很多句子，我们的拼音输入法就是要根据上下文找到一个最优的句子。 如果我们再将上下文的相关性量化，作为从前一个汉字到后一个汉字的距离，那么，寻找给 定拼音条件下最合理句子的问题就变成了一个典型的“最短路径”问题，我们的算法就是动态 规划。
